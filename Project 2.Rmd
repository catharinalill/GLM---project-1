---
title: "TMA4315: Compulsory exercise 2" 
subtitle: "Aurora Hofman, Camilla Karlsen, Catharina Lilleengen" 
date: "`r format(Sys.time(), '%d.%m.%Y')`"
output: html_document
---


# Problem 1

```{r setup, include=TRUE}
  filepath <- "https://www.math.ntnu.no/emner/TMA4315/2018h/mountains"
  mount <- read.table(file = filepath, header = TRUE, col.names = c("height", 
      "prominence", "fail", "success"))
  attach(mount)
```
##a) 
From the given dataset we see that we have group data, which means that every covariate vector, $\vec{x_i}$, in the design matrix $X$ that are identical, are grouped. In this case this means that everyone that has attempted to reach the same mountain falls in the same group. We assume that the number of groups $G$ is much less then the total number of observations $n = \sum \limits_{i=1}^{G} n_i$, where $n_i$ is the number of replicates of $\vec{x_i}$. This is reasonable to assume since there are far more people who have attempted reaching an mountain in our dataset, than the number of mountains which is 113 in this case. We also assume that each data $y_i$ are Bernoulli distributed with $P(y_i = 1) = \pi_i$. For the people attempting there is a certain probability of reaching each mountain which is $\pi_i$. So the attempt is either a success or fail. Hence, we have a binary model because we have two response values, the number of successes and number of fails. The response variables $y_i$ are assumed to be independent given det covariates $x_{i1},\ldots, x_{ik}$. The linear predictor in this case is 
$$
\eta_i = \beta_0 + \beta_1x_{i1} + \ldots + \beta_kx_{ik} = \mathbf{x_i}^T\mathbf{\beta}, 
$$
which is linked to $\pi_i$ via the response function, 
$$
  \pi_i = h(\eta_i)
$$
The canonical link function for a binary regression model is the logit model, which is given by
$$
  \pi_i = \frac{\exp({\eta_i})}{1+\exp({\eta_i})}.
$$
For the canonical link function the log-likelihood function is always concave so that if the maximum likelihood estimator exists it is always unique. Thus, we choose the logit  model as our link function. 

We now fit a glm modelling how the probability that an attempt at reaching a particular summit depends on its height and prominence. 
```{r, include=TRUE}
  mod <- glm(cbind(success,fail)~height+prominence, family=binomial(link = "logit"))
  summary(mod)
```

FLERE ASSUMPTIONS??

##b) 
By used of the observed deviance, we can estimate the overdispersion parameter $\phi$. We have, 
$$
  \hat{\phi} = \frac{D}{G-p}, 
$$
where $D$ is the deviance, $G$ is the number of groups and $p$ is the number of estimated regression coefficients. It the code below we estimate the observed deviance. 
```{r, include=TRUE}
  deviance <- summary(mod)$deviance
  G <- nrow(mount)      #number of groups
  p <- ncol(mount)-1    #number of estimated regression coefficients
  phi <- deviance/(G-p)
```
From the output we see that $\hat{phi}$ is larger than 1, thus we have overdispersion. Reasons for overdispersion can be unobserved heterogeneity and positive correlation between independent binary observations of the response variable. In this case, the data will be correlated because each individual most likely will belong to a cluster since reaching a mountain often happens in groups with a guide. Because we have overdispersion we want to refit the model using a quasi-likelihood model. 

```{r, include=TRUE}
  mod2 <- glm(cbind(success,fail)~height+prominence, family=quasibinomial(link = "logit"))
  summary(mod2)
```

##c) 
Here we want to choose the best model for our data. This means we compare models with all possible combinations of the covariates we have in our original model, hereby called the full model, and also including the model with only intercept. To compare the models we look at the QAIC criterion which is given by, 
$$
  QAIC = -\frac{2l(\hat{\theta})}{\hat{\phi}} +2p.
$$
We let $\hat{\phi}$ be a common estimate of the overdispersion parameter under the full model, and we count $\phi$ as a estimated parameter. To compute the QAIC for the different models we start by computing the likelihood function after we have fitted non-quasi-likelihood models for the differnt combinations. 
```{r, include=TRUE}
  library(ISwR)
  
  #Model selection with QAIC  
  #Full model
  #logliklihood = sum(dbinom(success,success+fail, mod$fitted.values,log=TRUE))
  likelihood = logLik(mod)
  
  QAIC_mod = -2*likelihood/phi+2*(p+1)
  QAIC_mod
  
  #Model with only height as covariate
  mod10 <- glm(cbind(success,fail)~height, family=binomial(link = "logit"))
  likelihood10 = logLik(mod10)
  QAIC_mod10 = -2*likelihood10/phi+2*(p)
  QAIC_mod10
  
  #Model with only prominence as covariate
  mod01 <- glm(cbind(success,fail)~prominence , family=binomial(link = "logit"))
  likelihood01 = logLik(mod01)
  QAIC_mod01 = -2*likelihood01/phi+2*(p)
  QAIC_mod01
  
  #Model with only intercept
  mod00 <- glm(cbind(success,fail)~1 , family=binomial(link = "logit"))
  likelihood00 = logLik(mod00)
  QAIC_mod00 = -2*likelihood00/phi+2*(p-1)
  QAIC_mod00
  
  #SKAL VI OGSÅ HA MED AIC FOR HVER MODEL HER??
```
By looking at the computed QAICs we see that the model with both covariates has the lowest value. Hence, we choose this model as our model. 

##d)
Now we want to test the significance of each term in the model using both Wald test and likelihood ratio test. 
```{r, include=TRUE}
drop1(mod, test = "LRT")  
summary(mod2)

#Wald test
beta <- coef(mod2)
d <- c(0,0,0)
C <- rbind(c(1,0,0),
           c(0,1,0),
           c(0,0,1))
wald <- t(C %*% beta - d) %*% solve(C %*% vcov(mod2) %*% t(C)) %*% (C %*% beta - d)
wald
pchisq(wald, df=3, lower.tail=FALSE)

#library(aod)
#wald.test(vcov(mod2),beta,c(1,2,3))
  
```
By use of likelihood ratio test we see that each term is significant in the model. From this test we can also see that both covariates is significant. By comparing with the summary from the fitted model we see that the covariate prominence is less significant. 

By use of the Wald test we see that the p-value is much less than any reasonable significance level. Hence, we reject the null hypothesis that the coefficient estimates are zero. Thus the coefficients are significant. 

"Given your choice of link function, give interpretations of the estimated regression slope parameters, in language that you would use to communicate to non-statisticians. " ?????????
intepretation of logit model, odds, chap 5.1 i boka

##e) 
```{r, include=TRUE}
library(ggplot2)
data = data.frame(fitted = mod2$fitted.values, res = residuals(mod2,type = "deviance"))
ggplot(data,aes(x=fitted,y = res)) + geom_point()

data2 = data.frame(height, res = residuals(mod2,type = "deviance"))
ggplot(data2,aes(x=height,y = res)) + geom_point()

data3 = data.frame(prominence, res = residuals(mod2,type = "deviance"))
ggplot(data3,aes(x=prominence,y = res)) + geom_point()

```
Dataene ser ikke normalfordelt ut, varianser ser ut til å øke, ikke heteroscedasisity


##f) 
```{r, include=TRUE}
alpha=0.05
sigma_matrix= vcov(mod2)
sds = sqrt(diag(sigma_matrix))
lower = mod2$coefficients - qnorm(1-alpha/2) * sds
upper = mod2$coefficients + qnorm(1-alpha/2) * sds

predlow = lower %*% c(1,8848,8848)
predupp = upper %*% c(1,8848,8848)
ci = c(predlow, predupp)
probci = plogis(ci) ##Translate the CI to probability scale
probci

#new = data.frame(height = 8848, prominence = 8848)
#predicted = predict(mod2, new)
#pred = plogis(predicted)

#logprobCI = confint(mod2,level = 0.95)
#predict(mod2, newdata=new, level = 0.95, interval = "confidence")

#ci=confint(mod2) #based on asymptotic normality

```

# Problem 2

```{r, include=TRUE}
 long <- read.csv("https://www.math.ntnu.no/emner/TMA4315/2019h/eliteserie.csv")
```

##a) 

##b) 

##c) 

##d) 

##e) 









