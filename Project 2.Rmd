---
title: "TMA4315: Compulsory exercise 2" 
subtitle: "Aurora Hofman, Camilla Karlsen, Catharina Lilleengen" 
date: "`r format(Sys.time(), '%d.%m.%Y')`"
output: html_document
---


# Problem 1

```{r setup, include=TRUE}
  filepath <- "https://www.math.ntnu.no/emner/TMA4315/2018h/mountains"
  mount <- read.table(file = filepath, header = TRUE, col.names = c("height", 
      "prominence", "fail", "success"))
  attach(mount)
```
##a) 
From the given dataset we see that we have group data, which means that every covariate vector, $\vec{x_i}$, in the design matrix $X$ that are identical, are grouped. In this case this means that everyone that has attempted to reach the same mountain falls in the same group. We assume that the number of groups $G$ is much less then the total number of observations $n = \sum \limits_{i=1}^{G} n_i$, where $n_i$ is the number of replicates of $\vec{x_i}$. This is reasonable to assume since there are far more people who have attempted reaching an mountain in our dataset, than the number of mountains which is 113 in this case. We also assume that each data $y_i$ are Bernoulli distributed with $P(y_i = 1) = \pi_i$. For the people attempting there is a certain probability of reaching each mountain which is $\pi_i$. So the attempt is either a success or fail. Hence, we have a binary model because we have two response values, the number of successes and number of fails. The response variables $y_i$ are assumed to be independent given the covariates $x_{i1},\ldots, x_{ik}$. The linear predictor in this case is 
$$
\eta_i = \beta_0 + \beta_1x_{i1} + \ldots + \beta_kx_{ik} = \mathbf{x_i}^T\mathbf{\beta}, 
$$
which is linked to $\pi_i$ via the response function, 
$$
  \pi_i = h(\eta_i).
$$
The canonical link function for a binary regression model is the logit model, which is given by
$$
  \pi_i = \frac{\exp({\eta_i})}{1+\exp({\eta_i})}.
$$
For the canonical link function the log-likelihood function is always concave so that if the maximum likelihood estimator exists it is always unique. Thus, we choose the logit  model as our link function. 

We now fit a glm modelling how the probability that an attempt at reaching a particular summit depends on its height and prominence. 
```{r, include=TRUE}
  mod <- glm(cbind(success,fail)~height+prominence, family=binomial(link = "logit"))
  summary(mod)
```

FLERE ASSUMPTIONS??

##b) 
By used of the observed deviance, we can estimate the overdispersion parameter $\phi$. We have, 
$$
  \hat{\phi} = \frac{D}{G-p}, 
$$
where $D$ is the deviance, $G$ is the number of groups and $p$ is the number of estimated regression coefficients. It the code below we estimate the observed deviance. 
```{r, include=TRUE}
  deviance <- summary(mod)$deviance
  G <- nrow(mount)      #number of groups
  p <- ncol(mount)-1    #number of estimated regression coefficients
  phi <- deviance/(G-p)
```
From the output we see that $\hat{\phi}$ is larger than 1, thus we have overdispersion. Reasons for overdispersion can be unobserved heterogeneity and positive correlation between independent binary observations of the response variable. In this case, the data will be correlated because each individual most likely will belong to a cluster since reaching a mountain often happens in groups with a guide. Because we have overdispersion we want to refit the model using a quasi-likelihood model. 

```{r, include=TRUE}
  mod2 <- glm(cbind(success,fail)~height+prominence, family=quasibinomial(link = "logit"))
  summary(mod2)
```

##c) 
Here we want to choose the best model for our data. This means we compare models with all possible combinations of the covariates we have in our original model, hereby called the full model, and also including the model with only intercept. To compare the models we look at the QAIC criterion which is given by, 
$$
  QAIC = -\frac{2l(\hat{\theta})}{\hat{\phi}} +2p.
$$
We let $\hat{\phi}$ be a common estimate of the overdispersion parameter under the full model, and we count $\phi$ as a estimated parameter. To compute the QAIC for the different models we start by computing the likelihood function after we have fitted non-quasi-likelihood models for the differnt combinations. 
```{r, include=TRUE}
  library(ISwR)

  #Model selection with QAIC   
  #Full model
  likelihood = logLik(mod)
  QAIC_mod = -2*likelihood/phi+2*(p+1)
  QAIC_mod
  
  #Model with only height as covariate
  mod10 <- glm(cbind(success,fail)~height, family=binomial(link = "logit"))
  likelihood10 = logLik(mod10)
  QAIC_mod10 = -2*likelihood10/phi+2*(p)
  QAIC_mod10
  
  #Model with only prominence as covariate
  mod01 <- glm(cbind(success,fail)~prominence , family=binomial(link = "logit"))
  likelihood01 = logLik(mod01)
  QAIC_mod01 = -2*likelihood01/phi+2*(p)
  QAIC_mod01
  
  #Model with only intercept
  mod00 <- glm(cbind(success,fail)~1 , family=binomial(link = "logit"))
  likelihood00 = logLik(mod00)
  QAIC_mod00 = -2*likelihood00/phi+2*(p-1)
  QAIC_mod00
  
  #SKAL VI OGSÅ HA MED AIC FOR HVER MODEL HER??
```
By looking at the computed QAICs we see that the model with both covariates has the lowest value. Hence, we choose this model as our model. 

##d)
Now we want to test the significance of each term in the model using both Wald test and likelihood ratio test. 
```{r, include=TRUE}
drop1(mod, test = "LRT")  #Skal det være mod2 her? 
summary(mod2)

#Wald test
beta <- coef(mod2)
d <- c(0,0,0)
C <- rbind(c(1,0,0),
           c(0,1,0),
           c(0,0,1))
wald <- t(C %*% beta - d) %*% solve(C %*% vcov(mod2) %*% t(C)) %*% (C %*% beta - d)
wald
pchisq(wald, df=3, lower.tail=FALSE)

#library(aod)
#wald.test(vcov(mod2),beta,c(1,2,3))
  
```
By use of likelihood ratio test we see that each term is significant in the model. From this test we can also see that both covariates is significant. By comparing with the summary from the fitted model we see that the covariate prominence is less significant. 

By use of the Wald test we see that the p-value is much less than any reasonable significance level. Hence, we reject the null hypothesis that the coefficient estimates are zero. Thus the coefficients are significant. 

"Given your choice of link function, give interpretations of the estimated regression slope parameters, in language that you would use to communicate to non-statisticians. " ?????????
intepretation of logit model, odds, chap 5.1 i boka

##e) 
```{r, include=TRUE}
library(ggplot2)
#Plotting the deviance residuals against fitted values
data = data.frame(fitted = mod2$fitted.values, res = residuals(mod2,type = "deviance"))
ggplot(data,aes(x=fitted,y = res)) + geom_point()
#Plotting the deviance residuals against the covariate height
data2 = data.frame(height, res = residuals(mod2,type = "deviance"))
ggplot(data2,aes(x=height,y = res)) + geom_point()
#Plotting the deviance residuals against the covariate prominence
data3 = data.frame(prominence, res = residuals(mod2,type = "deviance"))
ggplot(data3,aes(x=prominence,y = res)) + geom_point()

```
Examine the model fit.

Dataene ser ikke normalfordelt ut, varianser ser ut til å øke, ikke heteroscedasisity




##f) 
"The height and the prominence of Mount Everest are both 8848 meters. Compute a prediction for the probability that an attempt at this summit will be successful. First consider the predicted value and its variance on the scale of the linear predictor. Also compute a 95% confidence interval for the predicted value on this scale based on asymptotic normality of β̂ .Transform this confidence interval to the probability scale and explain the theory behind the transformation you're using.""
```{r, include=TRUE}
alpha=0.05
sigma_matrix= vcov(mod2) #the estimate variance matrix of β
sds = sqrt(diag(sigma_matrix))
lower = mod2$coefficients - qnorm(1-alpha/2) * sds
upper = mod2$coefficients + qnorm(1-alpha/2) * sds

predlow = lower %*% c(1,8848,8848)
predupp = upper %*% c(1,8848,8848)
ci = c(predlow, predupp) #95% confidence interval for the predicted value 
probci = plogis(ci) #Transform the confindence interval to probability scale
probci

#new = data.frame(height = 8848, prominence = 8848)
#predicted = predict(mod2, new)
#pred = plogis(predicted)

#logprobCI = confint(mod2,level = 0.95)
#predict(mod2, newdata=new, level = 0.95, interval = "confidence")

#ci=confint(mod2) #based on asymptotic normality

```
# Problem 2

```{r, include=TRUE}
long <- read.csv("https://www.math.ntnu.no/emner/TMA4315/2019h/eliteserie.csv")
```
##a)
```{r, include=TRUE}
mod <- glm(goals ~ attack + defence + home, poisson(link= log), data=long ) #log natural choice of link funk
summary(mod)
```

Velger log som link funksjon, naturlig for poisson, Dette antar at kovariatene har multiplikativ effekt på kovariatene. 

Antar at antall mål er poisson. 
Antar også uavhengighet mellom observasjonenen. Dette er ikke realistisk da to fotballag som spiller mot hverandre i stor grad vil påvirke hverandres sannsynlighet til å score mål. 
Antar også at antal scoringer kun er avhengig av hvem som er er i angrep, hvem som er i forvar og om man spiller på hjemmebane. Dette er en veldig enkel modell. Man kan tenke seg at vær, sykdom, ect kan spille inn


The estimates for attac implies how good a team is to score. This means one would want this to be large sice that gives more expected goals. 
The estimate for defence implies how good a team is to defend. This one would want to be negative. 
The estimate for home sais something about how playing at their homefield affects there ability to play well. 


##b)

If good teams both play well in attack and defence one would expect these parameters to be correlated. We do not expect corralation with the parameter home. 

```{r, include=TRUE}
formula = ~ attack + defence + home
#cor.test(formula, data=long)
cor.test(mod$coefficients[2:16], mod$coefficients[17:31]) #hva skjer med dummi parameteren.

#må også sjekke opp mot home
```


This supports the claim!!
  
##c)
  
```{r, include=TRUE}
deviance <- summary(mod)$deviance
n <- nrow(long)      #number of rows
p <- length(mod$coefficients) #number of estimated regression coefficients
phi <- deviance/ (352) #fyller inn manuelt da dataene inneholder uspilte kamper
phi

p_val = pchisq(deviance, 352, lower.tail = FALSE)
p_val

```

Ikke signigikant p verdi altså kan man ikke konkludere med overdispertion. Kunne tenke seg at kampene er korrelerte som kan føre til overdispurtion men vi kan altså ikke konkludere med dette. 

##d)

```{r, include=TRUE}
#lager funksjonen
team_rank <- function(predict_data,n){
  teams = predict_data$attack[1:16]
  teams_char = as.character(teams)
  zero <- rep(0,16)
  
  df <- data.frame("points" =zero , "conceded" = zero ,"goals"= zero, "diff" = zero)
  row.names(df) <- teams_char
  
  
  
  ### check goals and conceded
  for (i in (1:n)){
    attack = as.character(predict_data$attack[i])
    defence = as.character(predict_data$defence[i])
    goals = predict_data$goals[i]
    df[attack, "goals"] = df[attack, "goals"] + goals
    df[defence, "conceded"] = df[defence, "conceded"] + goals
    
  }
  
  for (i in seq(1,n, by=2)){
    attack = as.character(long$attack[i])
    defence = as.character(long$defence[i])
    goals_attack = predict_data$goals[i]
    goals_defence = predict_data$goals[i+1]
    if (goals_attack > goals_defence){
      df[attack, "points"] = df[attack, "points"] + 3
    }
    else if(goals_attack < goals_defence){
      df[defence, "points"] = df[defence, "points"] + 3
    }
    else{
      df[defence, "points"] = df[defence, "points"] + 1
      df[attack, "points"] = df[attack, "points"] + 1
    }
    
  }
  
  
  for (team in teams_char){
    df[team, "diff"] = df[team, "goals"]-df[team, "conceded"]
  }
  
  ranking = data.table::frank(df, -points, -diff, -goals, ties.method = "random")
  ranking
  
  return(ranking)
}

team_rank(long, 384) 

```

##e)
```{r, include=TRUE}
predicted_goals = (predict(mod, long, type="response"))
#predict_model
predict_long <- long ########## blir desimaltall, burde vært int!! 
predict_long$goals<-predicted_goals
n=480

team_rank(predict_long,n)

expected = (predict(mod, long, type="response"))
rank_matrix<- matrix(0,  16, 1000)
poisson_matrix <-matrix(0,  n, 1000)
set.seed(50)

for (i in (1:n)){
  poisson_matrix[i,] <- rpois(1000, expected[[i]])
}

predict_long <- long 
for (i in (1:1000)){
  predict_long$goals = poisson_matrix[,i]
  rank_matrix[,i] <-team_rank(predict_long,n)
}
```
```{r, include=TRUE}
#beregner gjennomsnittet
ex_fin_rank =rep(0,16)

for (i in (1:16)){
  sum = 0
  sum=sum(rank_matrix[i,])  
  sum = sum/1000
  ex_fin_rank[i] = sum
}

ex_fin_rank

```

##f)
```{r, include=TRUE}

C<- matrix(0,15,32)
for (i in (1:15)){
  C[i,i] = 1
  C[i, i+15] = -1
}

d <- rep(0,15)
betahat <- as.matrix(coef(mod))
betahat

dim(C)
dim(betahat)

wald <- t(C %*% (betahat) - d) %*% solve(C %*% vcov(mod) %*% t(C)) %*% (C %*% betahat - d)
wald
pchisq(wald, df=2, lower.tail=FALSE)
```








