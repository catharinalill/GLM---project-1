---
title: "TMA4315: Compulsory exercise 2" 
subtitle: "Aurora Hofman, Camilla Karlsen, Catharina Lilleengen" 
date: "`r format(Sys.time(), '%d.%m.%Y')`"
output: html_document
---


# Problem 1

```{r setup, include=TRUE}
  filepath <- "https://www.math.ntnu.no/emner/TMA4315/2018h/mountains"
  mount <- read.table(file = filepath, header = TRUE, col.names = c("height", 
      "prominence", "fail", "success"))
  attach(mount)
```
##a) 
From the given dataset we notice that we have grouped data, which means that every covariate vector, $\vec{x_i}$, in the design matrix $X$ that are identical, are grouped. In our case this means that everyone that has attempted to reach the same mountain falls in the same group. We assume that the number of groups $G$ is much less then the total number of observations $n = \sum \limits_{i=1}^{G} n_i$, where $n_i$ is the number of replicates of $\vec{x_i}$. This is reasonable to assume since there are far more people who have attempted reaching an mountain in our dataset, than the number of mountains which is 113 in this case. We also assume that each data $y_i$ are Bernoulli distributed with $P(y_i = 1) = \pi_i$. For the people attempting there is a certain probability of reaching each mountain which is $\pi_i$. So the attempt is either a success or fail. Hence, we have a binary model because we have two response values, the number of successes and number of fails. The response variables $y_i$ are assumed to be independent given the covariates $x_{i1},\ldots, x_{ik}$. The linear predictor in this case is 
$$
\eta_i = \beta_0 + \beta_1x_{i1} + \ldots + \beta_kx_{ik} = \mathbf{x_i}^T\mathbf{\beta}, 
$$
which is linked to $\pi_i$ via the response function, 
$$
  \pi_i = h(\eta_i).
$$
The canonical link function for a binary regression model is the logit model, which is given by
$$
  \pi_i = \frac{\exp({\eta_i})}{1+\exp({\eta_i})}.
$$
For the canonical link function the log-likelihood function is always concave so that if the maximum likelihood estimator exists it is always unique. Thus, we choose the logit  model as our link function. 

We now fit a glm modelling how the probability that an attempt at reaching a particular summit depends on its height and prominence. 
```{r, include=TRUE}
  mod <- glm(cbind(success,fail)~height+prominence, family=binomial(link = "logit"))
  summary(mod)
```

FLERE ASSUMPTIONS??

##b) 
By use of the observed deviance, we can estimate the overdispersion parameter $\phi$. We have, 
$$
  \hat{\phi} = \frac{D}{G-p}, 
$$
where $D$ is the deviance, $G$ is the number of groups and $p$ is the number of estimated regression coefficients. It the code below we estimate the observed deviance. 
```{r, include=TRUE}
  deviance <- summary(mod)$deviance
  G <- nrow(mount)      #number of groups
  p <- ncol(mount)-1    #number of estimated regression coefficients
  phi <- deviance/(G-p)
```
From the output we see that $\hat{\phi}$ is larger than 1, thus we have overdispersion. Reasons for overdispersion can be unobserved heterogeneity and positive correlation between independent binary observations of the response variable. In this case, the data will be correlated because each individual most likely will belong to a cluster since reaching a mountain often happens in groups with a guide. Because we have overdispersion we want to refit the model using a quasi-likelihood model. 

```{r, include=TRUE}
  mod2 <- glm(cbind(success,fail)~height+prominence, family=quasibinomial(link = "logit"))
  summary(mod2)
```

##c) 
Now we want to choose the best model for our data. This means we compare models with all possible combinations of the covariates we have in our original model, hereby called the full model, and also including the model with only intercept and choose the best of them. To compare the models we look at the QAIC criterion which is given by, 
$$
  QAIC = -\frac{2l(\hat{\theta})}{\hat{\phi}} +2p, 
$$
where $l(\hat{\theta})$ is the likelihood function evaluated at the estimated regression coefficients. We let $\hat{\phi}$ be a common estimate of the overdispersion parameter under the full model, and we count $\phi$ as a estimated parameter. To compute the QAIC for the different models we start by computing the likelihood function after we have fitted non-quasi-likelihood models for the different combinations og covariates. 
```{r, include=TRUE}
  library(ISwR)

  #Full model
  likelihood = logLik(mod)
  QAIC_mod = -2*likelihood/phi+2*(p+1)
  QAIC_mod
  
  #Model with only height as covariate
  mod10 <- glm(cbind(success,fail)~height, family=binomial(link = "logit"))
  likelihood10 = logLik(mod10)
  QAIC_mod10 = -2*likelihood10/phi+2*(p)
  QAIC_mod10
  
  #Model with only prominence as covariate
  mod01 <- glm(cbind(success,fail)~prominence , family=binomial(link = "logit"))
  likelihood01 = logLik(mod01)
  QAIC_mod01 = -2*likelihood01/phi+2*(p)
  QAIC_mod01
  
  #Model with only intercept
  mod00 <- glm(cbind(success,fail)~1 , family=binomial(link = "logit"))
  likelihood00 = logLik(mod00)
  QAIC_mod00 = -2*likelihood00/phi+2*(p-1)
  QAIC_mod00
  
  #SKAL VI OGSÅ HA MED AIC FOR HVER MODEL HER??
```
By looking at the computed QAICs we see that the model with both covariates has the lowest value. Hence, we choose the full model. 

##d)
Now we want to test the significance of each term in the model using both Wald test and likelihood ratio test. 
```{r, include=TRUE}
drop1(mod2, test = "LRT")  #Skal det være mod2 her? 
summary(mod2)

#Wald test
beta <- coef(mod2)
d <- c(0,0,0)
C <- rbind(c(1,0,0),
           c(0,1,0),
           c(0,0,1))
wald <- t(C %*% beta - d) %*% solve(C %*% vcov(mod2) %*% t(C)) %*% (C %*% beta - d)
wald
pchisq(wald, df=3, lower.tail=FALSE)
```
By use of likelihood ratio test we see that each term in the model is significant. By comparing with the summary from the fitted model we see that both covariates are a bit less significant in the summary output. 

By use of the Wald test we see that the p-value is much less than any reasonable significance level. Hence, we reject the null hypothesis that the coefficient estimates are zero. Thus the coefficients are significant. 

"Given your choice of link function, give interpretations of the estimated regression slope parameters, in language that you would use to communicate to non-statisticians. " ?????????
intepretation of logit model, odds, chap 5.1 i boka

Our choice of link function was the logit model. To interpret the estimated regression coefficients we look at the odds, 
$$
  \frac{\pi_i}{1-\pi_i} = \frac{P(y_i=1 | x_i)}{P(y_i=0 | x_i)} = \exp(\beta_0)\cdot \exp(x_{i1}\beta_1)\cdot \ldots \cdot \exp(x_{ik}\beta_k) = \exp(\eta_i). 
$$
So, if we for example increase $x_{i2}$ by one unit to $x_{i2}+1$  and keep all the other covariates constant, then we have
$$
  \frac{\pi_i}{1-\pi_i} = \exp(\beta_0)\cdot \exp(x_{i1}\beta_1)\cdot \exp((x_{i2}+1)\beta_2)\cdot \ldots \cdot \exp(x_{ik}\beta_k)
$$
$$
  = \exp(\beta_0)\cdot \exp(x_{i1}\beta_1)\cdot \exp(x_{i2}\beta_2)\cdot\exp(\beta_2)\cdot \ldots \cdot \exp(x_{ik}\beta_k) = \exp(\eta_i) \cdot\exp(\beta_2).
$$
Hence, the odds changes by $\exp(\beta_2)$. So, if $\beta_2 >0$ then $P(y_i=1)/P(y_i=0)$ increases and if $\beta_2 < 0$ then $P(y_i=1)/P(y_i=0)$ decreases. Also, if $\beta_2 = 0$ then $P(y_i=1)/P(y_i=0)$ remain unchanged. Hence, the effect of the covariates on the odds is exponentially multiplicative. 

This can also be interpreted in terms of the log-odds, 
$$
\log\left(\frac{\pi_i}{1-\pi_i}\right) = \eta_i = x_i^T\beta.
$$
We then have a linear relationship, and if we now increase $x_{i2}$ by one unit to $x_{i2}+1$ and keep all the other covariates constant, then the log-odds will change with a factor of $\beta_2$ as shown below. 
$$
\log\left(\frac{\pi_i}{1-\pi_i}\right) =  \beta_0 + \beta_1x_{i1} + \beta_2(x_{i2}+1) + \ldots + \beta_kx_{ik} = x_i^T\beta + \beta_2. 
$$
   
##e) 
```{r, include=TRUE}
library(ggplot2)
#Plotting the deviance residuals against fitted values
data = data.frame(fitted = mod2$fitted.values, res = residuals(mod2,type = "deviance"))
ggplot(data,aes(x=fitted,y = res)) + geom_point()
#Plotting the deviance residuals against the covariate height
data2 = data.frame(height, res = residuals(mod2,type = "deviance"))
ggplot(data2,aes(x=height,y = res)) + geom_point()
#Plotting the deviance residuals against the covariate prominence
data3 = data.frame(prominence, res = residuals(mod2,type = "deviance"))
ggplot(data3,aes(x=prominence,y = res)) + geom_point()

```

Examine the model fit.

When plotting the fitted values against the residuals one can see that the data points does not seem to be randomly spread around the horizontal axis $y=0$. Hence, we do not have a linear relationship between the response and the covariates?? It also seems like the variance increase, so we do not have heteroscedasisty. (Ehh, er det sånn man sier det??) se på tidligere statlær øvinger kanskje

The figures with the covariates prominence and height plotted against the residuals looks more randomized aroud the horizontal axis, and thus ... (Hvordan skal vi kommentere dette?)

Ser ikke ut for å være et lineært forhold mellom response og kovariater?, variansen ser ut til å øke, ikke heteroscedasisity, hmm heter det homoscedasisty hvis variansen er konstant kanskje, så vi har ikke homo...?



##f) 
"The height and the prominence of Mount Everest are both 8848 meters. Compute a prediction for the probability that an attempt at this summit will be successful. First consider the predicted value and its variance on the scale of the linear predictor. Also compute a 95% confidence interval for the predicted value on this scale based on asymptotic normality of β̂ .Transform this confidence interval to the probability scale and explain the theory behind the transformation you're using.""

The height and the prominence of Mount Everest are both 8848 meters. We start by computing a prediction for the probability that an attempt at the summit will be successfull. Then we compute a 95% confidence interval for the predicted value on this scale based on asymptotic normality of β. 

```{r, include=TRUE}
alpha=0.05
sigma_matrix= vcov(mod2) #the estimate variance matrix of β
sds = sqrt(diag(sigma_matrix))
lower = mod2$coefficients - qnorm(1-alpha/2) * sds
upper = mod2$coefficients + qnorm(1-alpha/2) * sds

predlow = lower %*% c(1,8848,8848)
predupp = upper %*% c(1,8848,8848)
ci = c(predlow, predupp) #95% confidence interval for the predicted value 
probci = plogis(ci) #Transform the confindence interval to probability scale
probci

#new = data.frame(height = 8848, prominence = 8848)
#predicted = predict(mod2, new)
#pred = plogis(predicted)

#logprobCI = confint(mod2,level = 0.95)
#predict(mod2, newdata=new, level = 0.95, interval = "confidence")

#ci=confint(mod2) #based on asymptotic normality

```

To transform the confidence interval we have used that, ...
   
# Problem 2

In this problem we want to use generalized linear model to analyse part of the 2018 results from the Norwegian elite football league. 
```{r, include=TRUE}
long <- read.csv("https://www.math.ntnu.no/emner/TMA4315/2019h/eliteserie.csv")
```
##a)
We start by fitting a model with attack, defence and home as covariates, to the data.
```{r, include=TRUE}
mod <- glm(goals ~ attack + defence + home, poisson(link= log), data=long ) #log natural choice of link funk
summary(mod)
```

We choose the log-linear model as our link function since this is the natural choice for a poisson distribution. Further we assume that the response variables $y_i$, i.e. number of goals, is poisson distributed with parameter $\lambda_i$. We assume that these are independent between the observations. This is not that realistic since two teams playing against each other probably will affect each others chance of making a goal. 

Moreover we assume that the only factors who influence the number of goals are the defence team, the attack team and weather they play on the home field or not. This is a very simple model and one could imagine that factors such as weather, illness, ect. can make a difference. 

The parameter in the model is given by
$$
  \lambda_i = \exp(x_i^T\beta) =\exp(\eta_i) ,
$$
or equivalently,
$$
  \log(\lambda_i) = x_i^T\beta.
$$
Hence, the effect of covariates on the rate $\lambda$ is similar to the effects on the odds in the logit model, which is exponentially multiplicative. 

The estimates for attack implies how good a team is to score. This means one would want this to be large since that gives more expected goals. 
The estimate for defence implies how good a team is to defend. This one would want to be negative. The estimate for home says something about how playing at their homefield affects their ability to play well. 


##b)

If good teams play well both in attack and defence one would expect these parameters to be positively correlated. We do not expect correlation with the parameter home. 

```{r, include=TRUE}
formula = ~ attack + defence + home
#cor.test(formula, data=long)
cor.test(mod$coefficients[2:16], mod$coefficients[17:31]) #hva skjer med dummi parameteren.

#må også sjekke opp mot home men da får man vektorer med forskjellig lengde! 
```
As one can see from the output above, the correlation is high. This supports the claim that attack and defence are positively correlated. 

Det er positiv korrelasjon her sant??? det er et negativt tall men attack og defence har alltid motsatt fortegn
     
##c)
We now want to check if there is overdispersion in the data. To do so we estimate the overdispersion parameter $\phi$, which is given in problem $2b)$. We know this parameter is chisquared distributed with $xxx$ degrees of freedom so we use this to calculate the p-value.   
```{r, include=TRUE}
deviance <- summary(mod)$deviance
n <- nrow(long)               #number of rows
p <- length(mod$coefficients) #number of estimated regression coefficients
phi <- deviance/ (352) #check summery to find degrees of freedom
phi

p_val = pchisq(deviance, 352, lower.tail = FALSE)
p_val

#HVORDAN KOMMER VI FREM TIL 352 her?? leser det fra summary på toppen :) 

```

This p-value is not significant and one can not conclude with overdispersion. Still possible sources of overdispersion could be correlated matches.

##d)
Now we want to make a function that gives us the teamrank based on the games played. 

```{r, include=TRUE}
#making the function
team_rank <- function(predict_data,n){
  teams = predict_data$attack[1:16]
  teams_char = as.character(teams)
  zero <- rep(0,16)
  
  df <- data.frame("points" =zero , "conceded" = zero ,"goals"= zero, "diff" = zero)
  row.names(df) <- teams_char
  
  
  
  ### check goals and conceded
  for (i in (1:n)){
    attack = as.character(predict_data$attack[i])
    defence = as.character(predict_data$defence[i])
    goals = predict_data$goals[i]
    df[attack, "goals"] = df[attack, "goals"] + goals
    df[defence, "conceded"] = df[defence, "conceded"] + goals
    
  }
  
  #count points
  for (i in seq(1,n, by=2)){
    attack = as.character(long$attack[i])
    defence = as.character(long$defence[i])
    goals_attack = predict_data$goals[i]
    goals_defence = predict_data$goals[i+1]
    if (goals_attack > goals_defence){
      df[attack, "points"] = df[attack, "points"] + 3
    }
    else if(goals_attack < goals_defence){
      df[defence, "points"] = df[defence, "points"] + 3
    }
    else{
      df[defence, "points"] = df[defence, "points"] + 1
      df[attack, "points"] = df[attack, "points"] + 1
    }
    
  }
  
  for (team in teams_char){
    df[team, "diff"] = df[team, "goals"]-df[team, "conceded"]
  }
  
  #get rank
  ranking = data.table::frank(df, -points, -diff, -goals, ties.method = "random")
  ranking
  
  return(ranking)
}

team_rank(long, 384) 

```

##e)

Now we want to simulate the entire tippeliga a thousand times and then compute the ranking in each case before computing an average ranking per team.  

```{r, include=TRUE}

#predicted_goals = (predict(mod, long, type="response"))
#predict_model
#predict_long <- long  
#predict_long$goals<-predicted_goals


#team_rank(predict_long,n)

#Find the betas from our model
expected = (predict(mod, long, type="response"))

n=480 #number of matches per tippeliga

#construct emty rank_matrixes to fill in the rank for each tippeliga and a poisson
rank_matrix<- matrix(0,  16, 1000)
#construct an emty matrix to fill inn the simuated results per game per tippeliga (480*1000) matrix
poisson_matrix <-matrix(0,  n, 1000)

set.seed(50)

#fill the poisson_matrix with simulations of number of goals scored per game, do this thousand times for each match
for (i in (1:n)){
  poisson_matrix[i,] <- rpois(1000, expected[[i]])
}

#construct the rank based on each tippeliga
predict_long <- long 
for (i in (1:1000)){
  predict_long$goals = poisson_matrix[,i]
  rank_matrix[,i] <-team_rank(predict_long,n)
}

```
```{r, include=TRUE}
#calculating mean
ex_fin_rank =rep(0,16)

for (i in (1:16)){
  sum = 0
  sum=sum(rank_matrix[i,])  
  sum = sum/1000
  ex_fin_rank[i] = sum
}



teams<-long$attack[1:16]
team_av_rank<-ex_fin_rank
av_result<- rbind(teams,team_av_rank) #Får ikke denne til å bli pen, vil ha lagnavn over snittet

av_result


```

Her må vi forklare litt mer hva vi gjør, men tror jeg trenger en gjennomgang av hva som skjer først :) :) Har fikset litt opp nå, ble det bedre? 

##f)

One could assume that if each team is equally good at defence and attack that these parameters therefore should be equal in absolute value but of opposite sign. This is a linear hypothesis because it can be written as $C\beta = d$. When the $\beta_{attack}$ =$-\beta_{defence}$ this simply gives a $C$ matrix with two ones on every row, one at the place for the teams attack and one at the place for the teams defence. 

In the code below, the construction of C is shown as well as a wald test of this hypotheses.

```{r, include=TRUE}

C<- matrix(0,15,32)
for (i in (1:15)){
  C[i,i] = 1
  C[i, i+15] = -1
}

d <- rep(0,15)
betahat <- as.matrix(coef(mod))
betahat

dim(C)
dim(betahat)

wald <- t(C %*% (betahat) - d) %*% solve(C %*% vcov(mod) %*% t(C)) %*% (C %*% betahat - d)
wald
pchisq(wald, df=2, lower.tail=FALSE)
```
As one can see this gives a low p-value and we reject the nullhypothesis for any reasonable significance level. Hence, the simplification of the model to assume that the attack and defence strengths of each team are equal in absolute value but with opposite sign is not reasonable. ?? Er det riktig å si, ja :) ? 
Hva med å skrive det over i stedet? "with significance $0.0051$" mye bedre!!!!! :) 






