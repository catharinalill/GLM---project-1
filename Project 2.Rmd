---
title: "TMA4315: Compulsory exercise 2" 
subtitle: "Aurora Hofman, Camilla Karlsen, Catharina Lilleengen" 
date: "`r format(Sys.time(), '%d.%m.%Y')`"
output: html_document
---


# Problem 1

```{r setup, include=TRUE}
  filepath <- "https://www.math.ntnu.no/emner/TMA4315/2018h/mountains"
  mount <- read.table(file = filepath, header = TRUE, col.names = c("height", 
      "prominence", "fail", "success"))
  attach(mount)
```
##a) 
From the given dataset we see that we have group data. Everyone that has attempted to reach the same mountain falls in the same group. We also have

First we fit a glm modelling how the probability that an attempt at reaching a particular summit depends on its height and prominence. 
```{r, include=TRUE}
  mod <- glm(cbind(success,fail)~height+prominence, family=binomial(link = "logit"))
  summary(mod)
```
Here we want to fit a binary regression 

Catharina skriver inn denne senere

##b) 
By used of the observed deviance, we can estimate the overdispersion parameter $\phi$. We have, 
$$
  \hat{\phi} = \frac{D}{G-p}, 
$$
where $D$ is the deviance, $G$ is the number of groups and $p$ is the number of estimated regression coefficients. 
```{r, include=TRUE}
  deviance <- summary(mod)$deviance
  G <- nrow(mount)      #number of groups
  p <- ncol(mount)-1    #number of estimated regression coefficients
  phi <- deviance/(G-p)
```
Since $phi$ is larger than 1 we have overdispersion. Reasons for overdispersion can be unobserved heterogeneity and positive correlation between independent binary observation of the response variable. In this case, the data will be correlated because each individual most likely will belong to a cluster since reaching a mountain often happens in groups with a guide. Because we have overdispersion of we want to refit the model using a quasi-likelihood model. 

```{r, include=TRUE}
  mod2 <- glm(cbind(success,fail)~height+prominence, family=quasibinomial(link = "logit"))
  summary(mod2)
```

##c) 
```{r, include=TRUE}
  library(ISwR)
  #install.packages("ISwR")

  #X = model.matrix(~height + prominence, data = mount)
  #beta = summary(mod)$coefficients[,1] 
  #eta = X%*%beta
  #likelihood = sum(success*eta - (success+fail)*log(1+exp(eta)))
  
  #Model selection with QAIC  
  #Full model
  #logliklihood = sum(dbinom(success,success+fail, mod$fitted.values,log=TRUE))
  likelihood = logLik(mod)
  
  QAIC_mod = -2*likelihood/phi+2*(p+1)
  QAIC_mod
  
  #Model with only height as covariate
  mod10 <- glm(cbind(success,fail)~height, family=binomial(link = "logit"))
  #deviance10 <- summary(mod10)$deviance
  #phi10 <- deviance10/(G-p-1)
  likelihood10 = logLik(mod10)
  QAIC_mod10 = -2*likelihood10/phi+2*(p)
  QAIC_mod10
  
  #Model with only prominence as covariate
  mod01 <- glm(cbind(success,fail)~prominence , family=binomial(link = "logit"))
  #deviance01 <- summary(mod01)$deviance
  #phi01 <- deviance01/(G-p-1)
  likelihood01 = logLik(mod01)
  QAIC_mod01 = -2*likelihood01/phi+2*(p)
  QAIC_mod01
  
  #Model with only intercept
  mod00 <- glm(cbind(success,fail)~1 , family=binomial(link = "logit"))
  #deviance00 <- summary(mod00)$deviance
  #phi00 <- deviance00/(G-p-2)
  likelihood00 = logLik(mod00)
  QAIC_mod00 = -2*likelihood00/phi+2*(p-1)
  QAIC_mod00
  
  #SKAL VI OGSÅ HA MED AIC FOR HVER MODEL HER??
```
By looking at the computed QAICs we see that the model with both covariates has the lowest value. Hence, we choose this model as our model. 

##d)
```{r, include=TRUE}
drop1(mod, test = "LRT")  
summary(mod2)

#Wald test
beta <- coef(mod2)
d <- c(0,0,0)
C <- rbind(c(1,0,0),
           c(0,1,0),
           c(0,0,1))
wald <- t(C %*% beta - d) %*% solve(C %*% vcov(mod2) %*% t(C)) %*% (C %*% beta - d)
wald
pchisq(wald, df=3, lower.tail=FALSE)

#library(aod)
#wald.test(vcov(mod2),beta,c(1,2,3))
  
```
By use of likelihood ratio test we see that each term is significant in the model. From this test we can also see that both covariates is significant. By comparing with the summary from the fitted model we see that the covariate prominence is less significant. 

By use of the Wald test we see that the p-value is much less than any reasonable significance level. Hence, we reject the null hypothesis that the coefficient estimates are zero. Thus the coefficients are significant. 

"Given your choice of link function, give interpretations of the estimated regression slope parameters, in language that you would use to communicate to non-statisticians. " ?????????
intepretation of logit model, odds, chap 5.1 i boka

##e) 
```{r, include=TRUE}
library(ggplot2)
data = data.frame(fitted = mod2$fitted.values, res = residuals(mod2,type = "deviance"))
ggplot(data,aes(x=fitted,y = res)) + geom_point()

data2 = data.frame(height, res = residuals(mod2,type = "deviance"))
ggplot(data2,aes(x=height,y = res)) + geom_point()

data3 = data.frame(prominence, res = residuals(mod2,type = "deviance"))
ggplot(data3,aes(x=prominence,y = res)) + geom_point()

```
Dataene ser ikke normalfordelt ut, varianser ser ut til å øke, ikke heteroscedasisity


##f) 
```{r, include=TRUE}
new = data.frame(height = 8848, prominence = 8848)
predicted = predict(mod2, newdata = new)
predicted
pred = plogis(predicted)
pred
sigma_matrix= vcov(mod2)

logprobCI = confint(mod2,level = 0.95)
logprobCI
logprobCI = confint(mod2)
#predict(mod2, newdata=new, level = 0.95, interval = "confidence")
logprobCI
probCI = exp(logprobCI)
probCI
```

# Problem 2

```{r, include=TRUE}
 long <- read.csv("https://www.math.ntnu.no/emner/TMA4315/2019h/eliteserie.csv")
```

##a) 

##b) 

##c) 

##d) 

##e) 









