---
title: "Project 3"
author: "Aurora Hofman, Camilla Karlsen, Catharina Lilleengen"
date: "`r format(Sys.time(), '%d.%m.%Y')`"
output: html_document
---

#Problem 1 

We are given the mixed model 
$$
  y_{ij} = \beta_0 + \gamma_i + \epsilon_{ij}, 
$$
where $\gamma_i$ are iid $\mathcal{N}(0,\tau^2)$ and $\epsilon_{ij}$ are  iid $\mathcal{N}(0,\sigma^2)$ for $i = 1,\ldots,m,  j = 1,\ldots n$. This means we have the same number of observations for each group. We can also write the model as
$$
   y_{ij} = \boldsymbol{x}_{ij}^T\boldsymbol{\beta} + \boldsymbol{u}_{ij}\boldsymbol{\gamma}_i + \epsilon_{ij},  
$$
where in this case $\boldsymbol{x}_{ij}^T = 1$ and $\boldsymbol{u}_{ij}^T = 1$. For each group $i = 1,\ldots,m$ we have
$$
  \boldsymbol{y_i} = \boldsymbol{X_i}\boldsymbol{\beta} + \boldsymbol{U_i}\boldsymbol{\gamma_i} + \boldsymbol{\epsilon_i}.  
$$
Here $\boldsymbol{y_i}$ is a n-dimensional vector of response values for group $i$, $\boldsymbol{X_i}$ is a $(n\times p)$-dimensional design matrix, and $p=1$ since we only have intercept in the model. Hence, $\boldsymbol{X_i}$ is a $(n\times 1)$-dimensional vector with only ones. $\boldsymbol{U_i}$ in this case is also a $(n\times 1)$-dimensional design matrix with only ones. The p-dimensional vector of fixed effects $\boldsymbol{\beta} = \beta_0$ in this case, and since we only have a random intercept model the vector of group-specific effects $\boldsymbol{\gamma_i}$ has dimension $(1\times 1)$. Moreover $\boldsymbol{\epsilon_i}$ is a n-dimensional vector of errors. 
   
The model can be expressed in matrix notation as,
$$
  \boldsymbol{Y} = \boldsymbol{X}\boldsymbol{\beta} + \boldsymbol{U}\boldsymbol{\gamma} + \boldsymbol{\epsilon}.
$$
Here $\boldsymbol{Y} = [y_1^T \ldots y_m^T]^T$, $\boldsymbol{X} = [x_1^T \ldots  x_m^T]^T$, $\boldsymbol{\gamma} = [\gamma_1 \ldots \gamma_m]^T$, $\boldsymbol{\epsilon}= [\epsilon_1^T \ldots \epsilon_m^T]^T$ and $\boldsymbol{U} = blockdiag(U_1, \ldots , U_m)$.
Here $\boldsymbol{X}$ and $\boldsymbol{U}$ are design matrices, $\boldsymbol{\beta}$ is the vector for fixed effects and $\boldsymbol{\gamma}$ the vector of random effects. Since we have $\gamma_i$ and $\epsilon_{ij}$ mutually independent by assumption of the linear mixed models we have 
$$
  \boldsymbol{\gamma} \sim N(0,G) \quad \textrm{and} \quad \boldsymbol{\epsilon} \sim N(0,R),  
$$
where $G = \tau^2I$ and $R = \sigma^2I$. 
   
We now implement a function called $\texttt{mylmm}$ that computes the maximum likelihood and restricted maximum likelihood estimates of the parameters in the given mixed model. 
   
```{r setup, include=TRUE}
library(lme4)
data <- read.csv("https://www.math.ntnu.no/emner/TMA4315/2019h/random-intercept.csv",
  colClasses=c("numeric","factor"))
attach(data)


```

```{r , include=TRUE}

#Defining functions for beta(V), V(theta), l_p(theta), l_r(theta)

beta <- function(V){
  beta=solve(t(X)%*%solve(V)%*%X)%*%t(X)%*%solve(V)%*%y
  return(beta)
}

#Covariance matrix
V <- function(theta){
  R <- theta[1]*diag(n*m)
  G <- theta[2]*diag(m)
  V = R + U%*%G%*%t(U)
  return(V)
}

#profile log-likelihood
l_p <- function(theta){
  V = V(theta)
  beta_est=beta(V)
  l_p = -1/2*(log(det(V))+(t(y-X%*%beta_est)%*%solve(V)%*%(y-X%*%beta_est)))
  return(l_p)
}
#restricted log-likelihood
l_r <- function(theta){
  l_r = l_p(theta)-1/2*log(det(t(X)%*%solve(V(theta))%*%X))
  return(l_r)
}
```

```{r , include=TRUE}

#Define constands and design matrices X and U
  m<-nlevels(group)  # number of clusters/individuals
  n<-length(y)/m # number of measurements within each cluster
  U <- model.matrix(~0 + group)
  X <- matrix(1,m*n)

#Define mylmm
mylmm <- function(y, group,REML = FALSE){ 
  estimates=rep(0,3) #beta, sigma, tau
  theta <- rep(1,2) #sigma, tau
  
  #find theta trough numerical maximisation
  if (REML == FALSE) {
    obj = optim(par = theta,fn = l_p,control = list(fnscale=-1))
  }
  else {
    obj = optim(par = theta,fn = l_r,control = list(fnscale=-1))
  }
  
  #Extract estimated theta
  theta = obj$par
  #Estimated beta 
  beta = beta(V(theta))
  
  estimates[1] = beta
  estimates[2] = sqrt(theta[1])
  estimates[3] = sqrt(theta[2])
  return(estimates)
}

#Check the estimates against computed values by lmer fitted with maximum likelihood 
mylmm(y,group, REML = FALSE)
lmer(y ~ (1|group), REML=FALSE) 

#Check the estimates against computed values by lmer fitted with restricted maximum likelihood
mylmm(y,group, REML=TRUE)
lmer(y ~ (1|group), REML=TRUE)

```
The maxmimum likelihood estimates from $\texttt{mylmm}$ are $\hat\beta=10,43$, $\hat\sigma=1,171$ and $\hat\tau=1,307$. When comparing with the computed values by lmer which are $\hat\beta=10,43$, $\hat\sigma=1,171$ and $\hat\tau=1,306$, we see that the estimates are almost the same. 
   
The restricted maximum likelihood estimates from $\texttt{mylmm}$ are $\hat\beta=10,43$, $\hat\sigma=1,171$ and $\hat\tau=1,391$. When comparing to the computed values by lmer $\hat\beta=10,43$, $\hat\sigma=1,171$ and $\hat\tau=1,391$ we see that these are also almost the same. 

#Problem 2
###a)

```{r}
library(lme4)
data <-cbpp
```

We will model tha data using a GLMM random intercept model with period as fixed effect and herd as a random effect on the intercept. We will use the logit choice of link function and we assume that the $incidence_{ij} = Y_{ij}$ is binomially distributed with parameter $size_{ij}$ and $\pi_{ij}(\beta, \gamma)$. Thus, we have $E(Y_{ij}) = size_{ij}\pi_{ij} = \mu_{ij}$. 

<!--
and $Var(Y_{ij}) = size_{ij}\pi_{ij}(1-\pi_{ij})$.
In mathematical notation we get

$$ 
  Y_{ij} = x_{ij}^T\beta + \gamma_{0,i} + \epsilon_{ij},
$$
with $\gamma_{0,i}$ iid $\mathcal{N}(0,\tau_0^2)$ and $\epsilon_{ij}$ iid $\mathcal{N}(0,\sigma^2)$.
-->

The random effect is added to the general linear predictior giving the new linear predictor which is, 
$$ 
  \eta_{ij} = x_{ij}^T\beta + \gamma_{0,i}
$$
The linear predictor is linked to mean via the logit link function $g$. That is, $g(\mu_{ij}) = \eta_{ij}$, such that we have   pi eller mu her inni g??? 
$$
  g(\mu_{ij}) = g(size_{ij}\pi_{ij}) = \frac{size_{ij}\pi_{ij}}{}
$$  
<!--
We have logit choice of link function $g(\pi_{ij}) $  which is as given in problem 1. 
$$
  g(\pi_{ij}) = \log(\frac{\pi_{ij}{1-\pi_{ij})
$$
-->
This model gives conditionally independent $Y_{ij}$s but they are not marginally independent.


###b)
We now want to fit the model by computing the marginal likelihood using Gauss-Hermite quadrature. We investigate how many quadrature points we need to reliably compute the MLEs of $\beta$ to a numerical accuracy of 0. 
```{r}
library(lme4)

#BRUKER QUADTRATURE POINTS MÅ TESTE UT FOR HVILKEN nAGQ DEN STABILISERER SEG
mod <- glmer(cbind(incidence, size-incidence)~period +(1|herd), family=binomial(link = "logit"), nAGQ = 10, data = cbpp)
#nAGQ?????

#burde stabilisere seg rundt 5/6
summary(mod)
print(fixef(mod), digits=15)

#nAGQ = 1, intercept = -1.39834286446214
#nAGQ = 2, intercept = -1.397945612319865
#nAGQ = 3, intercept = -1.398037319121611
#nAGQ = 4, intercept = -1.399265834284414
#nAGQ = 5, intercept = -1.39920188258569
#nAGQ = 6, intercept = -1.399211494824236
#nAGQ = 7, intercept = -1.399235140053781

n = 13
intercept = rep(0,n)
for (i in 1:n) {
  mod <- glmer(cbind(incidence, size-incidence)~period +(1|herd), family=binomial(link = "logit"), nAGQ = i, data = cbpp)
  intercept_n = fixef(mod)[1]
  
  if (i > 1) {
    intercept[i] = abs(intercept_n - intercept_o)
  }
  intercept_o =  intercept_n
}
nAGQ = c(1:n)
plot(nAGQ, intercept, log = "y", type = "l", col = "blue", ylab = "error")

```
   
By testing different number of quadrature points we see that 10 seems to be enough for the estimates of $\beta$ to stabilize. 

###c)
Here we want to refit the model by maximising the Laplace approximation of the marginal likelihood. This is done by using $nAGQ = 1$. 
```{r}
#Refitting model using Laplace
mod_laplace <- glmer(cbind(incidence, size-incidence)~period +(1|herd), family=binomial(link = "logit"), data = cbpp)

#laplace er det samme som å sette nAGQ =1. Det er også default setting i glmer, vi kommer til å se litt endringer i estimater og tau
summary(mod_laplace)
```
By comparing the summaries of the two fitted models, we can see only slightly changes in some of the estimates. For example the estimate of $\tau_0^2$ is a bit lower for the model fitted by using 10 quadratures points compared to the refitted model. Also the estimates for $\beta$ are slightly changed. 

###d)
We now refit the model by maximising the Laplace approximation of the REML likelihood. 
```{r}
#fitter med REML, skal gi mindre biased varianse (tror både tau og sigma skal bli nærmere virkelig verdi)
#Ml gir biased varians, den underfitter. Forventer at tau skal bli litt større med REML
library(glmmTMB)
mod_reml <- glmmTMB(cbind(incidence, size-incidence)~period +(1|herd), family=binomial(link = "logit"), data = cbpp, REML=TRUE)

summary(mod_reml)
```
As we can see from the summary output above, the estimate of $\tau_0^2$ is a bit larger than for the previous models. Why????

###e)
```{r}
mod0 <- glmer(cbind(incidence, size-incidence)~1 +(1|herd), family=binomial(link = "logit"), nAGQ = 10, data = cbpp)
anova=anova(mod0,mod)
anova

test=-2*(anova$logLik[1]-anova$logLik[2])
test
pchisq(test, df=3, lower.tail=FALSE)

modrandom <-glm(cbind(incidence, size-incidence)~period, family=binomial(link = "logit"),data = cbpp)
#summary(modrandom)

test2=-2*(logLik(modrandom)-logLik(mod))
test2
0.5*(pchisq(test2, df=5, lower.tail=FALSE)+pchisq(test2, df=4, lower.tail=FALSE))
```


###f)

```{r}
#herd: i
#period : j
#Hvilke modell skal vi bruke?
x_new = data.frame(period = 1, herd = 1)
beta = mod_reml$coefficients #funker ikke

x_1 = c(1,1)
x_2 = c(1,2)
pi_1 = exp()   #predict(mod_reml, period = 1) #include intercept and one covariate
pi_1
pi_2 = predict(c(1,2), mod_reml)
odds_1 = pi_1/(1-pi_1)
odds_2 = pi_2/(1-pi_2)
odds_ratio = odds_2/odds_1

```



###g)