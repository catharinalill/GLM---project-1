---
title: "Project 3"
author: "Aurora Hofman, Camilla Karlsen, Catharina Lilleengen"
date: "`r format(Sys.time(), '%d.%m.%Y')`"
output: html_document
---

#Problem 1 

We are given the mixed model 
$$
  y_{ij} = \beta_0 + \gamma_i + \epsilon_{ij}, 
$$
where $\gamma_i$ are iid $\mathcal{N}(0,\tau^2)$ and $\epsilon_{ij}$ are  iid $\mathcal{N}(0,\sigma^2)$ for $i = 1,\ldots,m,  j = 1,\ldots n$. This means we have the same number of observations for each group. We can also write the model as
$$
   y_{ij} = \boldsymbol{x}_{ij}^T\boldsymbol{\beta} + \boldsymbol{u}_{ij}\boldsymbol{\gamma}_i + \epsilon_{ij},  
$$
where in this case $\boldsymbol{x}_{ij}^T = 1$ and $\boldsymbol{u}_{ij}^T = 1$. For each group $i = 1,\ldots,m$ we have
$$
  \boldsymbol{y_i} = \boldsymbol{X_i}\boldsymbol{\beta} + \boldsymbol{U_i}\boldsymbol{\gamma_i} + \boldsymbol{\epsilon_i}.  
$$
Here $\boldsymbol{y_i}$ is a n-dimensional vector of response values for group $i$, $\boldsymbol{X_i}$ is a $(n\times p)$-dimensional design matrix, and $p=1$ since we only have intercept in the model. Hence, $\boldsymbol{X_i}$ is a $(n\times 1)$-dimensional vector with only ones. $\boldsymbol{U_i}$ in this case is also a $(n\times 1)$-dimensional design matrix with only ones. The p-dimensional vector of fixed effects $\boldsymbol{\beta} = \beta_0$ in this case, and since we only have a random intercept model the vector of group-specific effects $\boldsymbol{\gamma_i}$ has dimension $(1\times 1)$. Moreover $\boldsymbol{\epsilon_i}$ is a n-dimensional vector of errors. 
   
The model can be expressed in matrix notation as,
$$
  \boldsymbol{Y} = \boldsymbol{X}\boldsymbol{\beta} + \boldsymbol{U}\boldsymbol{\gamma} + \boldsymbol{\epsilon}.
$$
Here $\boldsymbol{Y} = [y_1^T \ldots y_m^T]^T$, $\boldsymbol{X} = [x_1^T \ldots  x_m^T]^T$, $\boldsymbol{\gamma} = [\gamma_1 \ldots \gamma_m]^T$, $\boldsymbol{\epsilon}= [\epsilon_1^T \ldots \epsilon_m^T]^T$ and $\boldsymbol{U} = blockdiag(U_1, \ldots , U_m)$.
Here $\boldsymbol{X}$ and $\boldsymbol{U}$ are design matrices, $\boldsymbol{\beta}$ is the vector for fixed effects and $\boldsymbol{\gamma}$ the vector of random effects. Since we have $\gamma_i$ and $\epsilon_{ij}$ mutually independent by assumption of the linear mixed models we have 
$$
  \boldsymbol{\gamma} \sim N(0,G) \quad \textrm{and} \quad \boldsymbol{\epsilon} \sim N(0,R),  
$$
where $G = \tau^2I$ and $R = \sigma^2I$. 
   
We now implement a function called $\texttt{mylmm}$ that computes the maximum likelihood and restricted maximum likelihood estimates of the parameters in the given mixed model. 
   
```{r setup, include=TRUE}
library(lme4)
data <- read.csv("https://www.math.ntnu.no/emner/TMA4315/2019h/random-intercept.csv",
  colClasses=c("numeric","factor"))
attach(data)


```

```{r , include=TRUE}

#Defining functions for beta(V), V(theta), l_p(theta), l_r(theta)

beta <- function(V){
  beta=solve(t(X)%*%solve(V)%*%X)%*%t(X)%*%solve(V)%*%y
  return(beta)
}

#Covariance matrix
V <- function(theta){
  R <- theta[1]*diag(n*m)
  G <- theta[2]*diag(m)
  V = R + U%*%G%*%t(U)
  return(V)
}

#profile log-likelihood
l_p <- function(theta){
  V = V(theta)
  beta_est=beta(V)
  l_p = -1/2*(log(det(V))+(t(y-X%*%beta_est)%*%solve(V)%*%(y-X%*%beta_est)))
  return(l_p)
}
#restricted log-likelihood
l_r <- function(theta){
  l_r = l_p(theta)-1/2*log(det(t(X)%*%solve(V(theta))%*%X))
  return(l_r)
}
```

```{r , include=TRUE}

#Define constands and design matrices X and U
  m<-nlevels(group)  # number of clusters/individuals
  n<-length(y)/m # number of measurements within each cluster
  U <- model.matrix(~0 + group)
  X <- matrix(1,m*n)

#Define mylmm
mylmm <- function(y, group,REML = FALSE){ 
  estimates=rep(0,3) #beta, sigma, tau
  theta <- rep(1,2) #sigma, tau
  
  #find theta trough numerical maximisation
  if (REML == FALSE) {
    obj = optim(par = theta,fn = l_p,control = list(fnscale=-1))
  }
  else {
    obj = optim(par = theta,fn = l_r,control = list(fnscale=-1))
  }
  
  #Extract estimated theta
  theta = obj$par
  #Estimated beta 
  beta = beta(V(theta))
  
  estimates[1] = beta
  estimates[2] = sqrt(theta[1])
  estimates[3] = sqrt(theta[2])
  return(estimates)
}

#Check the estimates against computed values by lmer fitted with maximum likelihood 
mylmm(y,group, REML = FALSE)
lmer(y ~ (1|group), REML=FALSE) 

#Check the estimates against computed values by lmer fitted with restricted maximum likelihood
mylmm(y,group, REML=TRUE)
lmer(y ~ (1|group), REML=TRUE)

```
The maxmimum likelihood estimates from $\texttt{mylmm}$ are $\hat\beta=10,43$, $\hat\sigma=1,171$ and $\hat\tau=1,307$. When comparing with the computed values by lmer which are $\hat\beta=10,43$, $\hat\sigma=1,171$ and $\hat\tau=1,306$, we see that the estimates are almost the same. 
   
The restricted maximum likelihood estimates from $\texttt{mylmm}$ are $\hat\beta=10,43$, $\hat\sigma=1,171$ and $\hat\tau=1,391$. When comparing to the computed values by lmer $\hat\beta=10,43$, $\hat\sigma=1,171$ and $\hat\tau=1,391$ we see that these are also almost the same. 

#Problem 2
###a)

```{r}
library(lme4)
data <-cbpp
```
We will model tha data using a GLMM random intercept model with period as fixed effect and herd as a random effect on the intercept. We will use the logit choice of link function and we assume that the $incidence_{ij} = Y_{ij}$ is binomially distributed with parameter $size_{ij}$ and $\pi_{ij}$, where $ i = 1,\ldots,15$ and $j = 1,\ldots,4$. Moreover we assume $Y_{ij} | \gamma_{0i} \sim bin(size_{ij},\pi_{ij})$, where $\gamma_{0i}$ is the random effects. This means we assume that given the random effects $\gamma_{0i}$ the responses $Y_{ij}$ are conditionally independent. Since we have grouped data we can consider the scaled response $\bar{Y}_{ij} = \frac{Y_{ij}}{size_{ij}}$. This means $\bar{Y}_{ij} | \gamma_i \sim \frac{bin(size_{ij},\pi_{ij})}{size_{ij}}$. Thus, we have $E(\bar{Y}_{ij}| \gamma_i) = \pi_{ij} = \mu_{ij}$. 

The random effect is added to the general linear predictior giving the new linear predictor which is, 
$$ 
  \eta_{ij} = x_{ij}^T\beta + \gamma_{0,i},
$$
where $\beta
The linear predictor is linked to mean via the logit link function $g$. That is, $g(\mu_{ij}) = \eta_{ij}$, such that we have
$$
  g(\mu_{ij}) = 
$$
<!--
We have logit choice of link function $g(\pi_{ij}) $  which is as given in problem 1. 
$$
  g(\pi_{ij}) = \log(\frac{\pi_{ij}{1-\pi_{ij})
$$
-->
This model gives conditionally independent $Y_{ij}$s but they are not marginally independent.
<!--
and $Var(Y_{ij}) = size_{ij}\pi_{ij}(1-\pi_{ij})$.
In mathematical notation we get

$$ 
  Y_{ij} = x_{ij}^T\beta + \gamma_{0,i} + \epsilon_{ij},
$$
with $\gamma_{0,i}$ iid $\mathcal{N}(0,\tau_0^2)$ and $\epsilon_{ij}$ iid $\mathcal{N}(0,\sigma^2)$.
-->

###b)
We now want to fit the model by computing the marginal likelihood using Gauss-Hermite quadrature. We investigate how many quadrature points we need to reliably compute the MLEs of $\beta$ to a numerical accuracy of 0. 

```{r}
library(lme4)
#Compute the marginal likelihood using Gauss-Hermite quadrature for different number of quadrature points, specified with the nAGQ-argument. 
n = 13
intercept = rep(0,n)
for (i in 1:n) {
  mod <- glmer(cbind(incidence, size-incidence)~period +(1|herd), family=binomial(link = "logit"), nAGQ = i, data = cbpp)
  intercept_n = fixef(mod)[1]
  
  if (i > 1) {
    intercept[i] = abs(intercept_n - intercept_o)
  }
  intercept_o =  intercept_n
}
nAGQ = c(1:n)

#Plot log10 of the absolute numerical error against nAGQ
plot(nAGQ, intercept, log = "y", type = "l", col = "blue", ylab = "error")

```
   
By testing different number of quadrature points we see that 10 seems to be enough for the estimates of $\beta$ to stabilize. 

```{r}
#Fit the model with 10 quadrature points
mod <- glmer(cbind(incidence, size-incidence)~period +(1|herd), family=binomial(link = "logit"), nAGQ = 10, data = cbpp)

summary(mod)
```

###c)
Here we want to refit the model by maximising the Laplace approximation of the marginal likelihood. This is done by using $nAGQ = 1$ which is the default for glmer. 
```{r}
#Refitting model using Laplace
mod_laplace <- glmer(cbind(incidence, size-incidence)~period +(1|herd), family=binomial(link = "logit"), data = cbpp)

summary(mod_laplace)
```
By comparing the summaries of the two fitted models, we can see only slightly changes in some of the estimates. For example the estimate of $\tau_0^2$ is a bit lower for the model fitted by using 10 quadratures points compared to the refitted model. Also the estimates for $\beta$ are slightly changed. 

###d)
We now refit the model by maximising the Laplace approximation of the REML likelihood. 
```{r}
#refitt model with REML
library(glmmTMB)
mod_reml <- glmmTMB(cbind(incidence, size-incidence)~period +(1|herd), family=binomial(link = "logit"), data = cbpp, REML=TRUE)

summary(mod_reml)
```
As we can see from the summary output above, the estimate of $\tau_0^2$ is a bit larger than for the previous models. This is due to that fitting the model with REML gives a less biased variance.   Hence, $\tau_0^2$ should be closer to its actual value. When fitting with maximum likelihood the variance gets biased and is underfitted. Hence, as expect the estimate for $\tau_0^2$ get a bit larger when using restricted maximum likelihood instead of maximum likelihood. 
How to further investigate ????

###e)
We test the significance of the random and fixed effects in the model using likelihood ratio tests and compute the associated p-values of the tests. 
```{r}
#Test for fixed effects
modfixed <- glmmTMB(cbind(incidence, size-incidence)~1 +(1|herd), family=binomial(link = "logit"), data = cbpp, REML=TRUE)
test1 = -2*(logLik(modfixed)-logLik(mod_reml))
p_1 = pchisq(test1, df=4, lower.tail=FALSE)
p_1 #p-value

#Test for random effects
modrandom <-glm(cbind(incidence, size-incidence)~period, family=binomial(link = "logit"),data = cbpp)
test2=-2*(logLik(modrandom)-logLik(mod_reml))
p_2 = 0.5*(pchisq(test2, df=0, lower.tail=FALSE)+pchisq(test2, df=1, lower.tail=FALSE))
p_2 #p-value
```

For the test of significans of fixed effects the null hypotheses is that all the betas are equal to zero. We therefor fitted a model without period (only intercept and random effects). This is our "small model". Then we extracted the loglikelihood for this model as well as for the full model ($H_1$) and used the formula for LRT test statistic. We get a very small p-value = $6.955269e-05$ so the nullhypotheses is rejected for level $0.05$. 

For the random effects we fitted a glm model without random effects (this is our "small" model) and used the likelihood for this model as well as for the full model to compute the LRT test statistic. The small model is in this case the null hypotheses. This is on the boundary of our parameter space. We then use the formula for computing the p value when the null hypotheses is on the boundary by using a mixture of the $\chi^2_0$ and the $\chi^2_1$ to get the correct p-value. We can see that we get a p-value = $0.0003192463$ so we reject $H_0$ and keep the random effects.   

###f)
We want to calculate the ratio between the odds that cattle develop CBPP wihtin a given herd for period 2 relative to period 1. We also want to calculate this odds-ratio for the changing in the incidence of CBPP on the level of all herds.
```{r}
#herd: i
#period : j
#Hvilke modell skal vi bruke?
x_new = data.frame(period = 1, herd = 1)
beta = fixef(mod_reml)$cond 
beta

x_1 = c(1,0,0,0)
x_2 = c(1,1,0,0)

#Odds ratio for a given herd
odds_ratio = exp(beta[2])
odds_ratio

#Odds ratio for all herds
tau = VarCorr(mod_reml)$cond$herd
tau = as.numeric(tau)
c = 1/sqrt(0.6*tau^2+1)
beta_new = c*beta[2]
odds_ratio = exp(beta_new)
odds_ratio

```
The effect on the log-odds is linear if we have a one unit change in the period. Hence, when changing the period with one unit, the effect on the odds is multiplicative. Thus, the ratio between odds for a given herd is 0.379 and the ratio for all herds is 0.402. This tells us that for a given herd there is a larger change in the odds, then for the whole population. 
??? logit/log(odds) = linear predictor???


###g)
Based on the estimated model we use simulations to find a numerical estimate of the intraclass correlation between the incidence of CBPP for a herd with fixed size 20 for period 1 and period 2.

```{r}
#bruker beta fra tidligere i oppgaven

b=summary(mod_reml)
b
beta[2]
beta[1]
#Extract tau from model
tau = b$varcor$cond$herd[1]

size = 20

#simulate 1000 gammas from this normal distribution
gammas = rnorm(1000, 0, tau) #skal denne vÃ¦re i annne 
gammas
#perios 1
pi_log_en = beta[1]  + gammas

#period 2
pi_log_to= beta[1] + beta[2] + gammas

#get the numbers for new insidences
Y_real_en = size* exp(pi_log_en)/(1 + exp(pi_log_en))
Y_real_to = size* exp(pi_log_to)/(1 + exp(pi_log_to))

#find the correlation
cor(Y_real_en, Y_real_to)

```
By simulating a thousand realisations of gammas and then computing the corresponding responses for a herd with fixed size 20 we compute the intraclass correlation between period one and two. We get a very high correlation ($0.997$). This means that the results from period one and two resemble each other a lot. This suggests that the random variable is of great importance. 

