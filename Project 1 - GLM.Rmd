---
title: "TMA4315: Compulsory exercise 1" 
subtitle: "Aurora Hofman, Camilla Karlsen, Catharina Lilleengen" 
date: "`r format(Sys.time(), '%d.%m.%Y')`"
output: html_document
---
# Expressions for the log likelihood, the score function and the Fisher information 
For the first part of this exercise we will derive expressions for the log likelihood, the score function and the Fisher information for the binary regression model with grouped data and the logit choice of link function.

If some of the covariate vectors $x_i$ in the design matrix $X$ in a regression model is identical, the data can be grouped. We then have $n_i$ replicates of vector $x_i$ where $i = \{1,2,...,G\}.$ For grouped binary response variables we then have $y_i \sim B(n_i,\pi_i)$, where $\pi_i = P(\frac{y_i}{n_i} = 1) = E(\frac{y_i}{n_i})$. Thus, the density function is given by   
$$
  f(y_i|\pi_i) = \binom{n_i}{y_i} \pi_i^{y_i}(1-\pi_i)^{n_i-y_i}.
$$
   
## The log likelihood function   

From the definition of the log likelihood function we have
$$
  L(\beta) = \prod_{i=1}^{G}L_i(\beta) = \prod_{i=1}^{G}f_i(y_i|\pi_i) =
  \prod_{i=1}^{G} \binom{n_i}{y_i} \pi_i^{y_i}(1-\pi_i)^{n_i-y_i}.
$$
We know $$
l(\beta) = \log(L(\beta)),
$$   
and thus we have
$$
l_i(\beta) = y_i \log (\pi_i) + (n_i-y_i) \log (1-\pi_i) + \log\left( \binom{n_i}{y_i}\right) 
$$
$$
=y_i \log (\pi_i) - y_i \log (1-\pi_i) + n_i \log(1-\pi_i) + \log\left( \binom{n_i}{y_i} \right)
$$
$$
= y_i \log (\frac{\pi}{1-\pi}) + n_i log(1-\pi)+\log\left(\binom{n_i}{y_i}\right)
$$
$$
= y_i \eta_i - n_i \log (1- e^{\eta_i}) +\log\left(\binom{n_i}{y_i}\right),
$$
where $\eta_i = x_i^T\beta$.
Finally we have
$$
l(\beta) = \sum_{i=1}^{G}(l_i) = \sum_{i=1}^{G}(y_i \eta_i - n_i \log (1- e^{\eta_i})+\log\left(\binom{n_i}{y_i} \right).
$$


## The score function
The score function is given by the derivate of the loglikelihood function with respect to $\beta$. Thus we have, 
$$ 
s(\beta)= \frac{\partial}{\partial\beta}l(\beta)=\sum_{i=1}^{G}
s_i(\beta), 
$$
where 
$$
s_i(\beta)=\frac{\partial}{\partial\beta_i}(y_i\eta_i-\log(1+e^{\eta_i})+\log\left(\binom{n_i}{y_i}\right)
=y_i\frac{\partial \eta_i}{\partial\beta_i}-\frac{\eta_i}{1+e^{\eta_i}}e^\eta_i\frac{\partial \eta_i}{\partial\beta_i}
$$
$$
=y_ix_i-\pi_in_ix_i=(y_i-n_i\pi_i)x_i. 
$$
Here we have used that $\frac{\partial \eta_i}{\partial\beta_i} = x_i$.
Hence, 
$$
s(\beta)=\sum_{i=1}^{G}
s_i(\beta)=\sum_{i=1}^{G}x_i(y_i-n_i\pi_i)=\sum_{i=1}^{G}n_ix_i(\bar{y}_i-\pi_i), 
$$
where $\bar{y_i} = y_i/n_i$. 
   
## Fisher information
From the definition of the Fisher information we have, 
$$
F_i (\beta) = E(s_i(\beta) [s_i(\beta)]^T).
$$
By inserting the score function we have
$$
F_i (\beta) = E(n_i x_i x_i^T(\bar{ y_i}-\pi_i)(\bar{ y_i}-\pi_i)^T x_i n_i)
= E(n_i^2 x_i x_i^T ( \bar{y_i}-\pi_i)^2)
$$


$$
= n_i^2 x_i x_i^T E(\bar{y_i}-\pi_i)^2
= n_i^2 x_i x_i^T E(\frac{y_i}{n_i} - \pi_i)^2
=n_i^2 x_i x_i^T E(\frac{y_i}{n_i} - E(\frac{y_i}{n_i}))^2
$$


$$
=n_i^2 x_i x_i^T Var(\frac{y_i}{n_i})
=n_i^2 x_i x_i^T \frac{1}{n_i^2} n_i \pi_i(1-\pi_i)
=x_i x_i^T n_i \pi_i (1-\pi_i),
$$
where we in the transition from the second to the third line have used that $Var(X) = E(X-E(X))^2$. Hence, the Fisher information is given by

$$
F(\beta) = \sum_{i=1}^{G} F_i(\beta) = \sum_{i=1}^{G}(x_i x_i^T n_i \pi_i (1-\pi_i))
$$

   
# Implementation of the Fisher scoring algorithm
In the second part of the exercise we have written a function that implements the Fisher scoring algorithm for the model given above. We know that to get convergence we need the initial values to be close to the exact solution. Hence, we have chosen the initial values for the regression coefficients to be xx, because they are close to the exact solution. 




```{r, include=TRUE}
library(investr)

myglm <- function(formula, dataset) {
  X = model.matrix(formula, data = dataset)
  beta =  matrix(c(-55, 25),2,1)
  #beta = matrix(0,ncol(dataset)-1)                      # Initial values
  
  y = data$y
  n = data$n
  m = length(n)
  q = ncol(dataset)-1
  i = 1
  epsilon = 1
  
  while (epsilon > 10^(-4)) {
    eta = X%*%beta                                      # Linear predictor
    pi = exp(eta)/(1+exp(eta))
    
    S = t(X)%*%(y-n*pi)                                 # Score function
    D = diag(c(n*pi*(1-pi)),nrow = length(y)) 
    F = (t(X)%*%D%*%X) 
    #F = F + diag(10^(-10), nrow = nrow(F))             # Add a small number to the diagonal of 
    F_inv  <- solve(F)                                  # the Fisher information matrix to be able                                                                                                  # to invert the matrix, not necesarily when 
                                                        #good values og beta are chosen
    print(i)
    i <- i+1
    
    beta_new <- beta + F_inv%*%S                        # Calculate new estimates for the 
    print(beta_new)                                     # regression coefficients 
    epsilon <- norm(beta_new - beta)/norm(beta)
    beta <- beta_new
    
  }
  eta <- X%*%beta
  y_bar <- y/n
  pi_hat <- exp(eta)/(1+exp(eta))

  stderr <- sqrt(diag(F_inv))                           # Standard error for regression                                                                                                               coefficients
  
  #Find deviance
  deviance <- 2*sum(dbinom(y,size = n, prob=y_bar, log = TRUE)-dbinom(y,size = n, prob=pi_hat, log = TRUE))
  
  coef <- cbind(beta, stderr)
  colnames(coef) <- c("beta", "stderr")
  result <- list(coef,deviance)
  return(result)
}
formula=~ldose
#myglm(formula,data = beetle)

summary(glm(cbind(y,n-y) ~ ldose, binomial, data=beetle))

#deviance(glm(cbind(y,n-y) ~ ldose, binomial, data=beetle)) #check if we have the correct deviance

```
Our model gives a deviance of $11.23$. When one looks at the definition of the deviance one can see that it is in fact the difference between logarithm of to probablities. Since probabilities are always between $(0,1)$ we can test with some numerical values. For example  one gets $log(0.7)-log(0.6) = 0.15$ even when this is multiplied by $8\cdot2$ which is the number of groups times to we get $2.46$ which is a lot smaller than our deviance. In other words if the cantidate model is very good one would expect a very low deviance. When testing $8 \cdot 2 \cdot (log(0.9)-log(0.4))$ one gets $12.97$. This implies that with a deviance of $11.23$ one can have quite a big difference between the candidate model and the saturated model implying that the model is not a very good one. 
